{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ef53bf",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-27T20:08:59.392823Z",
     "iopub.status.busy": "2024-11-27T20:08:59.392251Z",
     "iopub.status.idle": "2024-11-27T20:09:11.774526Z",
     "shell.execute_reply": "2024-11-27T20:09:11.772543Z"
    },
    "papermill": {
     "duration": 12.393067,
     "end_time": "2024-11-27T20:09:11.777178",
     "exception": false,
     "start_time": "2024-11-27T20:08:59.384111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting indic-transliteration\r\n",
      "  Downloading indic_transliteration-2.3.68-py3-none-any.whl.metadata (1.4 kB)\r\n",
      "Collecting backports.functools-lru-cache (from indic-transliteration)\r\n",
      "  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (2024.5.15)\r\n",
      "Requirement already satisfied: typer in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (0.12.3)\r\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from indic-transliteration) (0.10.2)\r\n",
      "Collecting roman (from indic-transliteration)\r\n",
      "  Downloading roman-4.2-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (8.1.7)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (4.12.2)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer->indic-transliteration) (13.7.1)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer->indic-transliteration) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer->indic-transliteration) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\r\n",
      "Downloading indic_transliteration-2.3.68-py3-none-any.whl (155 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\r\n",
      "Downloading roman-4.2-py3-none-any.whl (5.5 kB)\r\n",
      "Installing collected packages: roman, backports.functools-lru-cache, indic-transliteration\r\n",
      "Successfully installed backports.functools-lru-cache-2.0.0 indic-transliteration-2.3.68 roman-4.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install indic-transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba9bbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:09:11.791641Z",
     "iopub.status.busy": "2024-11-27T20:09:11.791231Z",
     "iopub.status.idle": "2024-11-27T20:09:26.408513Z",
     "shell.execute_reply": "2024-11-27T20:09:26.407106Z"
    },
    "papermill": {
     "duration": 14.628022,
     "end_time": "2024-11-27T20:09:26.411330",
     "exception": false,
     "start_time": "2024-11-27T20:09:11.783308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans==4.0.0-rc1\r\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\r\n",
      "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hstspreload-2024.11.1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\r\n",
      "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\r\n",
      "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\r\n",
      "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\r\n",
      "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\r\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\r\n",
      "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\r\n",
      "Downloading hstspreload-2024.11.1-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\r\n",
      "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Building wheels for collected packages: googletrans\r\n",
      "  Building wheel for googletrans (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=5e588c511c7c231574e047da92ae63669a25bf7018a4388234ac61fc4a4525d1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\r\n",
      "Successfully built googletrans\r\n",
      "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\r\n",
      "  Attempting uninstall: h11\r\n",
      "    Found existing installation: h11 0.14.0\r\n",
      "    Uninstalling h11-0.14.0:\r\n",
      "      Successfully uninstalled h11-0.14.0\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.7\r\n",
      "    Uninstalling idna-3.7:\r\n",
      "      Successfully uninstalled idna-3.7\r\n",
      "  Attempting uninstall: httpcore\r\n",
      "    Found existing installation: httpcore 1.0.5\r\n",
      "    Uninstalling httpcore-1.0.5:\r\n",
      "      Successfully uninstalled httpcore-1.0.5\r\n",
      "  Attempting uninstall: httpx\r\n",
      "    Found existing installation: httpx 0.27.0\r\n",
      "    Uninstalling httpx-0.27.0:\r\n",
      "      Successfully uninstalled httpx-0.27.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "fastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\r\n",
      "jupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\r\n",
      "jupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.11.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ff525f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:09:26.429554Z",
     "iopub.status.busy": "2024-11-27T20:09:26.429153Z",
     "iopub.status.idle": "2024-11-27T20:09:32.729468Z",
     "shell.execute_reply": "2024-11-27T20:09:32.728108Z"
    },
    "papermill": {
     "duration": 6.311736,
     "end_time": "2024-11-27T20:09:32.731321",
     "exception": true,
     "start_time": "2024-11-27T20:09:26.419585",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "StdinNotImplementedError",
     "evalue": "raw_input was called, but this frontend does not support input requests.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 73\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Step 1: Input sentence in English\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     input_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter a sentence in English: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Step 2: Translate to Hindi\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     hindi_sentence \u001b[38;5;241m=\u001b[39m translate_to_hindi(input_sentence)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1281\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1287\u001b[0m )\n",
      "\u001b[0;31mStdinNotImplementedError\u001b[0m: raw_input was called, but this frontend does not support input requests."
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "import random\n",
    "import spacy\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "# Initialize the Translator object\n",
    "translator = Translator()\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def translate_to_hindi(input_sentence):\n",
    "    \"\"\"Translate English sentence to Hindi.\"\"\"\n",
    "    try:\n",
    "        translated = translator.translate(input_sentence, src='en', dest='hi')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(\"Error in translation:\", e)\n",
    "        return None\n",
    "\n",
    "def transliterate_to_english(hindi_sentence):\n",
    "    \"\"\"Transliterate Hindi sentence to English using indic-transliteration.\"\"\"\n",
    "    try:\n",
    "        # Use indic-transliteration for better Hindi to English transliteration\n",
    "        transliterated = transliterate(hindi_sentence, sanscript.DEVANAGARI, sanscript.ITRANS)\n",
    "        return transliterated\n",
    "    except Exception as e:\n",
    "        print(\"Error in transliteration:\", e)\n",
    "        return None\n",
    "\n",
    "def pos_tagging(input_sentence):\n",
    "    \"\"\"POS tagging to identify word types (e.g., noun, verb).\"\"\"\n",
    "    doc = nlp(input_sentence)\n",
    "    return [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "def create_code_switched_sentence(input_english, transliterated_english):\n",
    "    \"\"\"Create a contextually accurate code-switched sentence.\"\"\"\n",
    "    input_words = input_english.split()\n",
    "    transliterated_words = transliterated_english.split()\n",
    "\n",
    "    # POS tagging to get the word types\n",
    "    input_tags = pos_tagging(input_english)\n",
    "    \n",
    "    mixed_sentence = []\n",
    "    \n",
    "    for i in range(max(len(input_words), len(transliterated_words))):\n",
    "        if i < len(input_words):\n",
    "            word, tag = input_tags[i]\n",
    "            switch_chance = random.random()\n",
    "\n",
    "            # Criteria for switching:\n",
    "            # - For nouns or adjectives, switch to Hindi\n",
    "            # - For verbs, keep in English (to maintain syntax)\n",
    "            if (tag in ['NOUN', 'ADJ'] and switch_chance > 0.3) or (tag == 'PROPN' and switch_chance > 0.5):\n",
    "                # Pick a word from transliterated sentence if criteria is met\n",
    "                if i < len(transliterated_words):\n",
    "                    mixed_sentence.append(transliterated_words[i])\n",
    "                else:\n",
    "                    mixed_sentence.append(word)\n",
    "            else:\n",
    "                mixed_sentence.append(word)\n",
    "\n",
    "        # In case the transliterated words are shorter, just add remaining words\n",
    "        if i >= len(input_words) and i < len(transliterated_words):\n",
    "            mixed_sentence.append(transliterated_words[i])\n",
    "\n",
    "    return ' '.join(mixed_sentence)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Step 1: Input sentence in English\n",
    "    input_sentence = input(\"Enter a sentence in English: \")\n",
    "    \n",
    "    # Step 2: Translate to Hindi\n",
    "    hindi_sentence = translate_to_hindi(input_sentence)\n",
    "    if not hindi_sentence:\n",
    "        return\n",
    "\n",
    "    print(\"Hindi translation:\", hindi_sentence)\n",
    "    \n",
    "    # Step 3: Transliterate Hindi back to English\n",
    "    transliterated_english = transliterate_to_english(hindi_sentence)\n",
    "    if not transliterated_english:\n",
    "        return\n",
    "    \n",
    "    print(\"Transliterated English:\", transliterated_english)\n",
    "    \n",
    "    # Step 4: Create contextually accurate code-switched sentence\n",
    "    code_switched_sentence = create_code_switched_sentence(input_sentence, transliterated_english)\n",
    "    print(\"Code-switched sentence:\", code_switched_sentence)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f84024f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T18:10:01.832210Z",
     "iopub.status.busy": "2024-11-27T18:10:01.831830Z",
     "iopub.status.idle": "2024-11-27T18:10:46.312823Z",
     "shell.execute_reply": "2024-11-27T18:10:46.311194Z",
     "shell.execute_reply.started": "2024-11-27T18:10:01.832178Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591d3bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T18:10:49.599231Z",
     "iopub.status.busy": "2024-11-27T18:10:49.598769Z",
     "iopub.status.idle": "2024-11-27T18:10:53.159101Z",
     "shell.execute_reply": "2024-11-27T18:10:53.158020Z",
     "shell.execute_reply.started": "2024-11-27T18:10:49.599188Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "# Load spaCy models for English and Hindi\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_hi = spacy.load(\"xx_ent_wiki_sm\")  # Using a multilingual model for Hindi\n",
    "\n",
    "# Sample bilingual word alignment (bwa) dictionary (English -> Hindi)\n",
    "bwa = {\n",
    "    'market': 'बाजार',\n",
    "    'fruits': 'फल',\n",
    "    'buy': 'खरीदना',\n",
    "    'going': 'जा रहा हूँ'\n",
    "}\n",
    "\n",
    "# Example Input sentences (Matrix Language: English, Embedded Language: Hindi)\n",
    "ms = \"I am going to the market to buy some fruits.\"\n",
    "es = \"मैं बाजार जा रहा हूँ फल खरीदने के लिए।\"\n",
    "\n",
    "# Sample POS tagging, NER, and Parse Tree structure (simplified)\n",
    "# In practice, this would be generated by parsing tools (spaCy or Stanford Parser)\n",
    "pt = [\n",
    "    ('I', 'PRON'),\n",
    "    ('am', 'AUX'),\n",
    "    ('going', 'VERB'),\n",
    "    ('to', 'ADP'),\n",
    "    ('the', 'DET'),\n",
    "    ('market', 'NOUN'),\n",
    "    ('to', 'PART'),\n",
    "    ('buy', 'VERB'),\n",
    "    ('some', 'DET'),\n",
    "    ('fruits', 'NOUN')\n",
    "]\n",
    "pos = [('I', 'PRON'), ('am', 'AUX'), ('going', 'VERB'), ('to', 'ADP'), ('the', 'DET'), ('market', 'NOUN'),\n",
    "       ('to', 'PART'), ('buy', 'VERB'), ('some', 'DET'), ('fruits', 'NOUN')]\n",
    "ner = [('market', 'GPE'), ('fruits', 'FOOD')]  # Example: NER tags (e.g., GPE for location, FOOD for fruits)\n",
    "\n",
    "def is_switchable(node, pos_tag):\n",
    "    \"\"\"Determine if a word can be switched according to POS tag and content word.\"\"\"\n",
    "    # Switch only nouns, verbs, and adjectives\n",
    "    switchable_tags = ['NOUN', 'VERB', 'ADJ']\n",
    "    if pos_tag in switchable_tags:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def generate_code_switched_sentence(bwa, ms, es, pt, pos, ner):\n",
    "    \"\"\"Generate synthetic code-switched sentence based on the algorithm.\"\"\"\n",
    "    # Convert ms (English) to list of words\n",
    "    ms_words = ms.split()\n",
    "\n",
    "    # Step 1: Handle Named Entities (NER)\n",
    "    for entity, entity_type in ner:\n",
    "        if entity in es:  # Check if translation exists in es (Embedded Language)\n",
    "            ms = ms.replace(entity, entity)  # Replace with the translated word (assumed to be available)\n",
    "\n",
    "    # Step 2: Process parsing tree nodes and POS tagging\n",
    "    code_switched_sentence = []\n",
    "    for idx, (word, tag) in enumerate(pt):\n",
    "        switch_label = False\n",
    "\n",
    "        # Step 6: Switch based on Matrix Language Theory (content words only)\n",
    "        if is_switchable(word, tag):\n",
    "            switch_label = True\n",
    "\n",
    "        # Step 7-12: Adjust switch_label based on POS\n",
    "        if switch_label:\n",
    "            # If lexicality is not in {noun, adjective, verb}, do not switch\n",
    "            if tag not in ['NOUN', 'ADJ', 'VERB']:\n",
    "                switch_label = False\n",
    "\n",
    "        # Step 13: Check if word in bilingual word alignment and apply translation\n",
    "        if switch_label:\n",
    "            if word in bwa:  # Check if word exists in bilingual word alignment (bwa)\n",
    "                # Replace word in ms with the translation from es\n",
    "                ms_words[idx] = bwa.get(word, word)\n",
    "\n",
    "        code_switched_sentence.append(ms_words[idx])\n",
    "\n",
    "    # Return the final sentence\n",
    "    return ' '.join(code_switched_sentence)\n",
    "\n",
    "# Main function to run the algorithm\n",
    "def main():\n",
    "    code_switched_sentence = generate_code_switched_sentence(bwa, ms, es, pt, pos, ner)\n",
    "    print(\"Code-switched Sentence:\", code_switched_sentence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04ce37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T18:20:21.972702Z",
     "iopub.status.busy": "2024-11-27T18:20:21.971594Z",
     "iopub.status.idle": "2024-11-27T18:20:22.964164Z",
     "shell.execute_reply": "2024-11-27T18:20:22.962869Z",
     "shell.execute_reply.started": "2024-11-27T18:20:21.972652Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from googletrans import Translator\n",
    "import random\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the Translator for translation and transliteration\n",
    "translator = Translator()\n",
    "\n",
    "# Example Input sentence (Matrix Language: English, Embedded Language: Hindi)\n",
    "ms = \"I am going to the market to buy some fruits.\"\n",
    "es = \"मैं बाजार जा रहा हूँ फल खरीदने के लिए।\"\n",
    "\n",
    "# Example bilingual word alignment (BWA) - here, English -> Hindi transliterations\n",
    "# Note: In practice, this can be fetched using a bilingual dictionary or translation model\n",
    "bwa = {\n",
    "    'market': 'bazar',\n",
    "    'fruits': 'phal',\n",
    "    'buy': 'khareedna',\n",
    "    'going': 'ja raha hoon'\n",
    "}\n",
    "\n",
    "# Generate synthetic code-switched sentence\n",
    "def is_switchable(word, pos_tag):\n",
    "    \"\"\"Determine if a word is switchable based on its POS tag.\"\"\"\n",
    "    switchable_tags = ['NOUN', 'VERB', 'ADJ']  # Only nouns, verbs, adjectives can be switched\n",
    "    return pos_tag in switchable_tags\n",
    "\n",
    "def transliterate_to_english(hindi_word):\n",
    "    \"\"\"Transliterate Hindi words to English using Google Translator.\"\"\"\n",
    "    try:\n",
    "        translated = translator.translate(hindi_word, src='hi', dest='en')\n",
    "        return translated.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error in transliteration: {e}\")\n",
    "        return hindi_word  # Return the original word if there's an error\n",
    "\n",
    "def generate_code_switched_sentence(bwa, ms, es, nlp_model, translator):\n",
    "    \"\"\"Generate synthetic code-switched sentence based on the algorithm.\"\"\"\n",
    "    # Parse the English sentence using spaCy to get POS, NER, and parse tree structure\n",
    "    doc = nlp_model(ms)\n",
    "    \n",
    "    # Convert ms (Matrix Language: English) to list of words\n",
    "    ms_words = ms.split()\n",
    "    switched_words = []\n",
    "    \n",
    "    # Step 1: Handle Named Entities (NER) (simplified assumption)\n",
    "    for ent in doc.ents:\n",
    "        if ent.text.lower() in bwa:  # Check if translation exists in BWA\n",
    "            # Replace entity in ms with its transliterated version from BWA\n",
    "            ms_words = [bwa.get(word.lower(), word) for word in ms_words]\n",
    "    \n",
    "    # Step 2: Process parsing tree nodes and POS tagging for code-switching\n",
    "    for word, tag in zip(ms_words, [token.pos_ for token in doc]):\n",
    "        switch_label = False\n",
    "        \n",
    "        # Step 6: Switch based on Matrix Language Theory (content words only)\n",
    "        if is_switchable(word, tag):\n",
    "            switch_label = True\n",
    "        \n",
    "        # Step 7-12: Adjust switch_label based on POS (Only content words allowed to switch)\n",
    "        if switch_label:\n",
    "            # If word exists in BWA, replace it with transliteration\n",
    "            if word.lower() in bwa:\n",
    "                transliterated_word = transliterate_to_english(bwa.get(word.lower(), word))\n",
    "                switched_words.append(transliterated_word)\n",
    "            else:\n",
    "                switched_words.append(word)\n",
    "        else:\n",
    "            switched_words.append(word)\n",
    "    \n",
    "    # Return the final sentence\n",
    "    return ' '.join(switched_words)\n",
    "\n",
    "# Main function to run the algorithm\n",
    "def main():\n",
    "    code_switched_sentence = generate_code_switched_sentence(bwa, ms, es, nlp_en, translator)\n",
    "    print(\"Code-switched Sentence:\", code_switched_sentence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73156c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T18:29:06.377656Z",
     "iopub.status.busy": "2024-11-27T18:29:06.377228Z",
     "iopub.status.idle": "2024-11-27T18:29:15.038145Z",
     "shell.execute_reply": "2024-11-27T18:29:15.036862Z",
     "shell.execute_reply.started": "2024-11-27T18:29:06.377617Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from indic_transliteration import sanscript\n",
    "from indic_transliteration.sanscript import transliterate\n",
    "\n",
    "# Load spaCy model for English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to check if the word should be replaced based on its POS\n",
    "def should_switch(word, pos_tag):\n",
    "    \"\"\"Only switch nouns, verbs, and adjectives.\"\"\"\n",
    "    switchable_tags = ['NOUN', 'VERB', 'ADJ']\n",
    "    return pos_tag in switchable_tags\n",
    "\n",
    "# Function to transliterate English word to Hindi (Roman script)\n",
    "def transliterate_word(word):\n",
    "    \"\"\"Transliterate English word to Hindi in Roman script.\"\"\"\n",
    "    try:\n",
    "        # Transliterate using indic-transliteration library\n",
    "        transliterated_word = transliterate(word, sanscript.IAST, sanscript.DEVANAGARI)\n",
    "        return transliterated_word\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transliteration: {e}\")\n",
    "        return word  # Return the original word if error occurs\n",
    "\n",
    "# Function to generate a code-switched sentence using BWA and POS tagging\n",
    "def generate_code_switched_sentence(input_sentence):\n",
    "    # Process the input sentence using spaCy\n",
    "    doc = nlp(input_sentence)\n",
    "    \n",
    "    # List to hold the words of the code-switched sentence\n",
    "    code_switched_words = []\n",
    "    \n",
    "    # Iterate through each word in the sentence with its POS tag\n",
    "    for token in doc:\n",
    "        word = token.text\n",
    "        pos_tag = token.pos_\n",
    "        \n",
    "        # Apply code-switching only to nouns, verbs, and adjectives\n",
    "        if should_switch(word, pos_tag):\n",
    "            transliterated_word = transliterate_word(word)\n",
    "            code_switched_words.append(transliterated_word)\n",
    "        else:\n",
    "            # Keep non-switchable words (e.g., conjunctions, prepositions) in English\n",
    "            code_switched_words.append(word)\n",
    "    \n",
    "    # Return the modified sentence with code-switching\n",
    "    return ' '.join(code_switched_words)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Input sentence in English (Matrix Language)\n",
    "    input_sentence = input(\"Enter an English sentence: \")\n",
    "    \n",
    "    # Generate the code-switched sentence\n",
    "    code_switched_sentence = generate_code_switched_sentence(input_sentence)\n",
    "    \n",
    "    # Output the result\n",
    "    print(\"Code-switched Sentence:\", code_switched_sentence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83be9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T18:45:22.711245Z",
     "iopub.status.busy": "2024-11-27T18:45:22.710767Z",
     "iopub.status.idle": "2024-11-27T18:46:38.453715Z",
     "shell.execute_reply": "2024-11-27T18:46:38.452197Z",
     "shell.execute_reply.started": "2024-11-27T18:45:22.711206Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ai4bharat-transliteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ae455",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c973f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:16:19.544915Z",
     "iopub.status.busy": "2024-11-27T19:16:19.544463Z",
     "iopub.status.idle": "2024-11-27T19:16:26.533312Z",
     "shell.execute_reply": "2024-11-27T19:16:26.532178Z",
     "shell.execute_reply.started": "2024-11-27T19:16:19.544874Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_sentence= input(\"write a sentence\")\n",
    "translator = Translator()\n",
    "try:\n",
    "    translated = translator.translate(input_sentence, src='en', dest='hi')\n",
    "    print(translated.text)\n",
    "except Exception as e:\n",
    "    print(\"Error in translation:\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34facc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Transliteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e8ad2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:16:30.322509Z",
     "iopub.status.busy": "2024-11-27T19:16:30.322022Z",
     "iopub.status.idle": "2024-11-27T19:16:33.223728Z",
     "shell.execute_reply": "2024-11-27T19:16:33.222458Z",
     "shell.execute_reply.started": "2024-11-27T19:16:30.322467Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "try:\n",
    "    translated = translator.translate(input_sentence, src='en', dest='hi')\n",
    "    print(translated.text)\n",
    "except Exception as e:\n",
    "    print(\"Error in translation:\", e)\n",
    "from ai4bharat.transliteration import XlitEngine\n",
    "e = XlitEngine(src_script_type=\"indic\", beam_width=10, rescore=False)\n",
    "out = e.translit_sentence(translated.text, lang_code=\"hi\")\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e59ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:16:35.805623Z",
     "iopub.status.busy": "2024-11-27T19:16:35.805168Z",
     "iopub.status.idle": "2024-11-27T19:16:35.812785Z",
     "shell.execute_reply": "2024-11-27T19:16:35.811128Z",
     "shell.execute_reply.started": "2024-11-27T19:16:35.805582Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(input_sentence + \"--------->\" +out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e5411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:22:25.327713Z",
     "iopub.status.busy": "2024-11-27T19:22:25.327154Z",
     "iopub.status.idle": "2024-11-27T19:22:29.217679Z",
     "shell.execute_reply": "2024-11-27T19:22:29.216466Z",
     "shell.execute_reply.started": "2024-11-27T19:22:25.327669Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "# Load the spaCy multilingual model that can handle Hindi as well as English transliteration\n",
    "nlp_en = spacy.load('en_core_web_sm')  # English model\n",
    "nlp_hi = spacy.load('xx_ent_wiki_sm')  # Multilingual model for Hindi transliteration\n",
    "\n",
    "# Example Hindi to English dictionary (expand this as needed)\n",
    "hindi_to_english_dict = {\n",
    "    \"bajar\": \"market\",\n",
    "    \"phal\": \"fruits\",\n",
    "    \"khareedane\": \"buy\",\n",
    "    \"liye\": \"for\",\n",
    "    \"jaa\": \"go\",\n",
    "    \"rahahaa\": \"going\",\n",
    "    \"huun\": \"am\",\n",
    "    \"kuchh\": \"some\",\n",
    "}\n",
    "\n",
    "def contextual_code_switch(input_sentence, transliterated_sentence):\n",
    "    # Tokenize and POS tag both sentences (input: English, output: Hindi transliterated)\n",
    "    input_doc = nlp_en(input_sentence)  # POS tag for English\n",
    "    transliterated_doc = nlp_hi(transliterated_sentence)  # POS tag for Hindi transliterated\n",
    "    \n",
    "    input_tokens = input_sentence.split()\n",
    "    transliterated_tokens = transliterated_sentence.split()\n",
    "\n",
    "    # Initialize an empty list for the code-switched sentence\n",
    "    code_switched_sentence = []\n",
    "\n",
    "    # Max length to ensure we do not go out of bounds\n",
    "    max_len = max(len(input_tokens), len(transliterated_tokens))\n",
    "\n",
    "    # Iterate over the tokens to blend the sentences\n",
    "    for i in range(max_len):\n",
    "        if i < len(input_tokens) and i < len(transliterated_tokens):\n",
    "            # Get the POS tag of the current word in the original sentence and transliterated sentence\n",
    "            current_pos_tag_input = input_doc[i].pos_ if i < len(input_doc) else None\n",
    "            current_pos_tag_transliterated = transliterated_doc[i].pos_ if i < len(transliterated_doc) else None\n",
    "\n",
    "            # Decision-making for code-switching based on POS tags and the dictionary\n",
    "            if current_pos_tag_input in ['VERB', 'AUX', 'ADP', 'ADV']:  # Keep verbs and auxiliaries in English\n",
    "                code_switched_sentence.append(input_tokens[i])  # Keep in English\n",
    "            elif current_pos_tag_input in ['NOUN', 'PROPN']:  # Nouns can be switched\n",
    "                # For nouns, check if there's a mapping in the dictionary\n",
    "                word_in_dict = hindi_to_english_dict.get(transliterated_tokens[i], None)\n",
    "                if word_in_dict:\n",
    "                    code_switched_sentence.append(word_in_dict)  # Map to English if present in the dictionary\n",
    "                else:\n",
    "                    code_switched_sentence.append(transliterated_tokens[i])  # Otherwise, keep the transliterated word\n",
    "            elif current_pos_tag_input in ['DET', 'PRON']:  # Determiners and pronouns often stay in English\n",
    "                code_switched_sentence.append(input_tokens[i])\n",
    "            else:\n",
    "                # For adjectives, adverbs, and conjunctions, we randomly decide based on dictionary mapping\n",
    "                if random.random() < 0.5:\n",
    "                    code_switched_sentence.append(input_tokens[i])\n",
    "                else:\n",
    "                    word_in_dict = hindi_to_english_dict.get(transliterated_tokens[i], None)\n",
    "                    if word_in_dict:\n",
    "                        code_switched_sentence.append(word_in_dict)\n",
    "                    else:\n",
    "                        code_switched_sentence.append(transliterated_tokens[i])\n",
    "\n",
    "        elif i < len(input_tokens):\n",
    "            # If only the input has remaining tokens\n",
    "            code_switched_sentence.append(input_tokens[i])\n",
    "        elif i < len(transliterated_tokens):\n",
    "            # If only the transliterated has remaining tokens\n",
    "            word_in_dict = hindi_to_english_dict.get(transliterated_tokens[i], None)\n",
    "            if word_in_dict:\n",
    "                code_switched_sentence.append(word_in_dict)\n",
    "            else:\n",
    "                code_switched_sentence.append(transliterated_tokens[i])\n",
    "\n",
    "    # Join the tokens to form the final sentence\n",
    "    return ' '.join(code_switched_sentence)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"i am going to market to buy some fruits\"\n",
    "out = \"main kuchh phal khareedane key liye bajar jaa rahaa huun\"\n",
    "\n",
    "# Call the function to get the code-switched sentence\n",
    "result = contextual_code_switch(input_sentence, out)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf029d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:34:58.778172Z",
     "iopub.status.busy": "2024-11-27T19:34:58.777698Z",
     "iopub.status.idle": "2024-11-27T19:34:59.581491Z",
     "shell.execute_reply": "2024-11-27T19:34:59.580153Z",
     "shell.execute_reply.started": "2024-11-27T19:34:58.778131Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "class CustomDictionaryCodeSwitcher:\n",
    "    def __init__(self, custom_dictionary=None):\n",
    "        # Load English NLP model\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Use provided dictionary or default to empty\n",
    "        self.word_mapping = custom_dictionary or {}\n",
    "    \n",
    "    def generate_code_switched_sentence(self, matrix_sentence, switch_probability=0.5):\n",
    "        \"\"\"\n",
    "        Generate a code-switched sentence using custom dictionary\n",
    "        \n",
    "        Args:\n",
    "        matrix_sentence (str): Original sentence to be code-switched\n",
    "        switch_probability (float): Probability of switching a word\n",
    "        \n",
    "        Returns:\n",
    "        str: Code-switched sentence\n",
    "        \"\"\"\n",
    "        # Parse matrix language sentence\n",
    "        doc = self.nlp(matrix_sentence)\n",
    "        \n",
    "        # Create the code-switched sentence\n",
    "        code_switched_tokens = []\n",
    "        for token in doc:\n",
    "            # Check if token exists in mapping and should be switched\n",
    "            if (token.text in self.word_mapping and \n",
    "                random.random() < switch_probability):\n",
    "                code_switched_tokens.append(self.word_mapping[token.text])\n",
    "            else:\n",
    "                code_switched_tokens.append(token.text)\n",
    "        \n",
    "        return ' '.join(code_switched_tokens)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Custom dictionary for code-switching\n",
    "    custom_dict = {\n",
    "        \"market\": \"bajar\",\n",
    "        \"fruits\": \"phal\",\n",
    "        \"going\": \"jaa\",\n",
    "        \"buying\": \"khareedne\",\n",
    "        \"to\": \"ko\",\n",
    "        \"some\": \"kuchh\"\n",
    "    }\n",
    "    \n",
    "    # Input sentence\n",
    "    matrix_sentence = \"i am going to the market for buying some fruits\"\n",
    "    \n",
    "    # Initialize code-switch generator with custom dictionary\n",
    "    code_switcher = CustomDictionaryCodeSwitcher(custom_dict)\n",
    "    \n",
    "    # Generate code-switched sentence\n",
    "    code_switched = code_switcher.generate_code_switched_sentence(matrix_sentence)\n",
    "    \n",
    "    print(\"Original Sentence:\", matrix_sentence)\n",
    "    print(\"Code-Switched Sentence:\", code_switched)\n",
    "    print(\"\\nUsed Dictionary:\", custom_dict)\n",
    "\n",
    "# Uncomment to run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3bc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:43:50.551441Z",
     "iopub.status.busy": "2024-11-27T19:43:50.551030Z",
     "iopub.status.idle": "2024-11-27T19:43:51.304108Z",
     "shell.execute_reply": "2024-11-27T19:43:51.302960Z",
     "shell.execute_reply.started": "2024-11-27T19:43:50.551388Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "import re\n",
    "\n",
    "class EnhancedCodeSwitcher:\n",
    "    def __init__(self, custom_dictionary=None, nlp_model=\"en_core_web_sm\"):\n",
    "        \"\"\"\n",
    "        Initialize the code-switching generator\n",
    "        \n",
    "        Args:\n",
    "        custom_dictionary (dict): Custom word mapping\n",
    "        nlp_model (str): SpaCy language model to use\n",
    "        \"\"\"\n",
    "        # Load NLP model\n",
    "        try:\n",
    "            self.nlp = spacy.load(nlp_model)\n",
    "        except OSError:\n",
    "            print(f\"Warning: Model {nlp_model} not found. Please download it.\")\n",
    "            raise\n",
    "        \n",
    "        # Initialize dictionary\n",
    "        self.word_mapping = custom_dictionary or {}\n",
    "        \n",
    "        # Additional linguistic features\n",
    "        self.pos_switch_preferences = {\n",
    "            'NOUN': 0.7,     # High preference for switching nouns\n",
    "            'VERB': 0.6,     # Moderate preference for verbs\n",
    "            'ADJ': 0.5,      # Moderate preference for adjectives\n",
    "            'PROPN': 0.4,    # Low preference for proper nouns\n",
    "            'ADV': 0.3,      # Low preference for adverbs\n",
    "            'default': 0.2   # Low preference for other parts of speech\n",
    "        }\n",
    "    \n",
    "    def _preprocess_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Preprocess sentence to handle special cases\n",
    "        \n",
    "        Args:\n",
    "        sentence (str): Input sentence\n",
    "        \n",
    "        Returns:\n",
    "        str: Preprocessed sentence\n",
    "        \"\"\"\n",
    "        # Remove extra whitespaces\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "        return sentence\n",
    "    \n",
    "    def _get_pos_switch_probability(self, token):\n",
    "        \"\"\"\n",
    "        Determine switching probability based on part of speech\n",
    "        \n",
    "        Args:\n",
    "        token (spacy.tokens.Token): SpaCy token\n",
    "        \n",
    "        Returns:\n",
    "        float: Probability of switching\n",
    "        \"\"\"\n",
    "        return self.pos_switch_preferences.get(token.pos_, \n",
    "                                               self.pos_switch_preferences['default'])\n",
    "    \n",
    "    def generate_code_switched_sentence(\n",
    "        self, \n",
    "        matrix_sentence, \n",
    "        global_switch_probability=0.5, \n",
    "        context_aware=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a sophisticated code-switched sentence\n",
    "        \n",
    "        Args:\n",
    "        matrix_sentence (str): Original sentence to be code-switched\n",
    "        global_switch_probability (float): Overall switching likelihood\n",
    "        context_aware (bool): Enable context-aware switching\n",
    "        \n",
    "        Returns:\n",
    "        str: Code-switched sentence\n",
    "        \"\"\"\n",
    "        # Preprocess sentence\n",
    "        matrix_sentence = self._preprocess_sentence(matrix_sentence)\n",
    "        \n",
    "        # Parse matrix language sentence\n",
    "        doc = self.nlp(matrix_sentence)\n",
    "        \n",
    "        # Create the code-switched sentence\n",
    "        code_switched_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Determine switching probability\n",
    "            pos_switch_prob = (\n",
    "                self._get_pos_switch_probability(token) * \n",
    "                global_switch_probability\n",
    "            )\n",
    "            \n",
    "            # Context-aware switching\n",
    "            if (context_aware and \n",
    "                token.text in self.word_mapping and \n",
    "                random.random() < pos_switch_prob):\n",
    "                \n",
    "                # Switch the word\n",
    "                switched_word = self.word_mapping[token.text]\n",
    "                code_switched_tokens.append(switched_word)\n",
    "            else:\n",
    "                # Keep original word\n",
    "                code_switched_tokens.append(token.text)\n",
    "        \n",
    "        return ' '.join(code_switched_tokens)\n",
    "    \n",
    "    def add_word_mapping(self, new_mappings):\n",
    "        \"\"\"\n",
    "        Add new word mappings to the existing dictionary\n",
    "        \n",
    "        Args:\n",
    "        new_mappings (dict): Additional word mappings\n",
    "        \"\"\"\n",
    "        self.word_mapping.update(new_mappings)\n",
    "    \n",
    "    def get_word_mappings(self):\n",
    "        \"\"\"\n",
    "        Retrieve current word mappings\n",
    "        \n",
    "        Returns:\n",
    "        dict: Current word mappings\n",
    "        \"\"\"\n",
    "        return self.word_mapping\n",
    "\n",
    "# Example usage and demonstration\n",
    "def main():\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(420)\n",
    "    \n",
    "    # Comprehensive dictionary for code-switching\n",
    "    code_switch_dict = {\n",
    "        # Nouns\n",
    "        \"market\": \"bajar\",\n",
    "        \"fruits\": \"phal\",\n",
    "        \"home\": \"ghar\",\n",
    "        \"books\": \"kitab\",\n",
    "        \n",
    "        # Verbs\n",
    "        \"going to\": \"jaa rahahu\",\n",
    "        \"buy\": \"khareedne\",\n",
    "        \"read\": \"padhna\",\n",
    "        \n",
    "        # Adjectives\n",
    "        \"good\": \"achcha\",\n",
    "        \"big\": \"bada\",\n",
    "        \n",
    "        # Prepositions and other words\n",
    "        \"to\": \"ko\",\n",
    "        \"some\": \"kuchh\",\n",
    "        \"very\": \"bahut\"\n",
    "    }\n",
    "    \n",
    "    # Initialize code-switch generator\n",
    "    code_switcher = EnhancedCodeSwitcher(code_switch_dict)\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"i am going to market to buy some fruits, but I also have to buy books\",\n",
    "        \"i have a good book at home\",\n",
    "        \"he is reading a very big book\"\n",
    "    ]\n",
    "    \n",
    "    # Generate code-switched sentences\n",
    "    print(\"Code-Switched Sentences:\")\n",
    "    for sentence in test_sentences:\n",
    "        code_switched = code_switcher.generate_code_switched_sentence(\n",
    "            sentence, \n",
    "            global_switch_probability=0.8\n",
    "        )\n",
    "        print(f\"Original: {sentence}\")\n",
    "        print(f\"Switched: {code_switched}\\n\")\n",
    "    \n",
    "    # Demonstrate adding new mappings\n",
    "    code_switcher.add_word_mapping({\n",
    "        \"school\": \"school\",\n",
    "        \"computer\": \"computer\"\n",
    "    })\n",
    "\n",
    "# Uncomment to run\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c299079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T19:53:39.654323Z",
     "iopub.status.busy": "2024-11-27T19:53:39.653779Z",
     "iopub.status.idle": "2024-11-27T19:53:40.438238Z",
     "shell.execute_reply": "2024-11-27T19:53:40.437010Z",
     "shell.execute_reply.started": "2024-11-27T19:53:39.654277Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "class AlgorithmicCodeSwitcher:\n",
    "    def __init__(self, word_alignments=None):\n",
    "        \"\"\"\n",
    "        Initialize the code-switching generator\n",
    "        \n",
    "        Args:\n",
    "        word_alignments (dict): Bilingual word alignment dictionary\n",
    "        \"\"\"\n",
    "        # Load English and multi-language NLP models\n",
    "        self.nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Word alignment dictionary\n",
    "        self.word_alignments = word_alignments or {}\n",
    "    \n",
    "    def _is_switchable_node(self, token):\n",
    "        \"\"\"\n",
    "        Determine if a node is switchable based on Matrix Language Theory\n",
    "        \n",
    "        Args:\n",
    "        token (spacy.tokens.Token): SpaCy token\n",
    "        \n",
    "        Returns:\n",
    "        bool: Whether the token is switchable\n",
    "        \"\"\"\n",
    "        # Switchable parts of speech\n",
    "        switchable_pos = ['NOUN', 'ADJ', 'VERB', 'PROPN']\n",
    "        \n",
    "        # Check if token's POS is in switchable list\n",
    "        return token.pos_ in switchable_pos\n",
    "    \n",
    "    def generate_code_switched_sentence(\n",
    "        self, \n",
    "        matrix_sentence, \n",
    "        embedded_sentence\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a code-switched sentence using algorithmic approach\n",
    "        \n",
    "        Args:\n",
    "        matrix_sentence (str): Input sentence in matrix language\n",
    "        embedded_sentence (str): Input sentence in embedded language\n",
    "        \n",
    "        Returns:\n",
    "        str: Code-switched sentence\n",
    "        \"\"\"\n",
    "        # Step 1: Parse matrix language sentence\n",
    "        ms_doc = self.nlp_en(matrix_sentence)\n",
    "        \n",
    "        # Tokenize embedded sentence\n",
    "        embedded_tokens = embedded_sentence.split()\n",
    "        \n",
    "        # Initialize output tokens\n",
    "        code_switched_tokens = []\n",
    "        \n",
    "        # Step 2: Named Entity Replacement\n",
    "        for token in ms_doc:\n",
    "            # Check if token is a Named Entity and has a translation\n",
    "            if (token.ent_type_ and \n",
    "                token.text in self.word_alignments):\n",
    "                # Replace Named Entity\n",
    "                code_switched_tokens.append(\n",
    "                    self.word_alignments[token.text]\n",
    "                )\n",
    "            else:\n",
    "                code_switched_tokens.append(token.text)\n",
    "        \n",
    "        # Step 3: Parsing Tree and Switchability Analysis\n",
    "        for i, token in enumerate(ms_doc):\n",
    "            # Check if node is switchable\n",
    "            if self._is_switchable_node(token):\n",
    "                # Step 4: Word Alignment Replacement\n",
    "                if (token.text in self.word_alignments and \n",
    "                    i < len(embedded_tokens)):\n",
    "                    # Replace with aligned word or embedded language token\n",
    "                    replacement = (\n",
    "                        self.word_alignments.get(token.text, \n",
    "                        embedded_tokens[i])\n",
    "                    )\n",
    "                    code_switched_tokens[i] = replacement\n",
    "        \n",
    "        return ' '.join(code_switched_tokens)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Bilingual word alignment dictionary\n",
    "    word_alignments = {\n",
    "        # Named Entities\n",
    "        \"John\": \"John\",\n",
    "        \"New York\": \"New York\",\n",
    "        \n",
    "        # Other word mappings\n",
    "        \"market\": \"Bajar\",\n",
    "        \"going\": \"ja rahahu\",\n",
    "        \"buy\": \"kharedne\",\n",
    "        \"fruits\": \"phal\",\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Initialize code-switch generator\n",
    "    code_switcher = AlgorithmicCodeSwitcher(word_alignments)\n",
    "    \n",
    "    # Test sentences\n",
    "    matrix_sentence = \"i am going to market to buy some fruits\"\n",
    "    embedded_sentence = \"main kuchh phal khareedane key liye bajar jaa rahaa huun\"\n",
    "    \n",
    "    # Generate code-switched sentence\n",
    "    code_switched = code_switcher.generate_code_switched_sentence(\n",
    "        matrix_sentence, \n",
    "        embedded_sentence\n",
    "    )\n",
    "    \n",
    "    print(\"Matrix Language Sentence:\", matrix_sentence)\n",
    "    print(\"Embedded Language Sentence:\", embedded_sentence)\n",
    "    print(\"Code-Switched Sentence:\", code_switched)\n",
    "\n",
    "# Uncomment to run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e47c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-27T20:04:04.199098Z",
     "iopub.status.busy": "2024-11-27T20:04:04.198635Z",
     "iopub.status.idle": "2024-11-27T20:04:05.014299Z",
     "shell.execute_reply": "2024-11-27T20:04:05.012972Z",
     "shell.execute_reply.started": "2024-11-27T20:04:04.199063Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "class NaturalCodeSwitcher:\n",
    "    def __init__(self, word_alignments=None, switch_probability=0.3):\n",
    "        \"\"\"\n",
    "        Initialize the natural code-switching generator.\n",
    "        \n",
    "        Args:\n",
    "        word_alignments (dict): Bilingual word alignment dictionary.\n",
    "        switch_probability (float): Probability of switching eligible words.\n",
    "        \"\"\"\n",
    "        # Load English NLP model\n",
    "        self.nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Word alignment dictionary\n",
    "        self.word_alignments = word_alignments or {}\n",
    "        \n",
    "        # Switching probability\n",
    "        self.switch_probability = switch_probability\n",
    "\n",
    "    def _is_switchable_node(self, token):\n",
    "        \"\"\"\n",
    "        Determine if a node is switchable based on Matrix Language Theory.\n",
    "        \n",
    "        Args:\n",
    "        token (spacy.tokens.Token): SpaCy token.\n",
    "        \n",
    "        Returns:\n",
    "        bool: Whether the token is switchable.\n",
    "        \"\"\"\n",
    "        # Switchable parts of speech\n",
    "        switchable_pos = ['NOUN', 'ADJ', 'VERB', 'PROPN']\n",
    "        \n",
    "        # Check if token's POS is in switchable list\n",
    "        return token.pos_ in switchable_pos and not token.is_stop\n",
    "\n",
    "    def _switch_token(self, token, embedded_tokens, index):\n",
    "        \"\"\"\n",
    "        Switch a token to the embedded language based on context.\n",
    "        \n",
    "        Args:\n",
    "        token (spacy.tokens.Token): Token to be switched.\n",
    "        embedded_tokens (list): Tokens in the embedded language.\n",
    "        index (int): Current token index.\n",
    "        \n",
    "        Returns:\n",
    "        str: Switched or original token.\n",
    "        \"\"\"\n",
    "        if token.text in self.word_alignments:\n",
    "            # Replace with aligned word\n",
    "            return self.word_alignments[token.text]\n",
    "        elif index < len(embedded_tokens):\n",
    "            # Replace with embedded language token\n",
    "            return embedded_tokens[index]\n",
    "        return token.text\n",
    "\n",
    "    def generate_code_switched_sentence(\n",
    "        self, \n",
    "        matrix_sentence, \n",
    "        embedded_sentence\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate a code-switched sentence using an improved algorithm.\n",
    "        \n",
    "        Args:\n",
    "        matrix_sentence (str): Input sentence in matrix language.\n",
    "        embedded_sentence (str): Input sentence in embedded language.\n",
    "        \n",
    "        Returns:\n",
    "        str: Code-switched sentence.\n",
    "        \"\"\"\n",
    "        # Parse the matrix language sentence\n",
    "        ms_doc = self.nlp_en(matrix_sentence)\n",
    "        \n",
    "        # Tokenize embedded sentence\n",
    "        embedded_tokens = embedded_sentence.split()\n",
    "        \n",
    "        # Initialize output tokens\n",
    "        code_switched_tokens = []\n",
    "        \n",
    "        for i, token in enumerate(ms_doc):\n",
    "            if self._is_switchable_node(token) and random.random() < self.switch_probability:\n",
    "                # Attempt to switch token\n",
    "                switched_token = self._switch_token(token, embedded_tokens, i)\n",
    "                code_switched_tokens.append(switched_token)\n",
    "            else:\n",
    "                # Keep original token\n",
    "                code_switched_tokens.append(token.text)\n",
    "        \n",
    "        # Join tokens into a sentence\n",
    "        return ' '.join(code_switched_tokens)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Bilingual word alignment dictionary\n",
    "    word_alignments = {\n",
    "        \"market\": \"bazar\",\n",
    "        \"going\": \"ja raha hoon\",\n",
    "        \"buy\": \"kharidne\",\n",
    "        \"fruits\": \"phal\",\n",
    "        \"some\": \"kuchh\",\n",
    "        \"to\": \"ke liye\",\n",
    "   \"kids\": \"bachche\",\n",
    "    \"are\": \"hain\",\n",
    "    \"playing\": \"khel rahe\",\n",
    "    \"in\": \"mein\",\n",
    "    \"garden\": \"bagiche\",\n",
    "    \"with\": \"ke saath\",\n",
    "    \"friends\": \"doston\",\n",
    "            \"I\": \"mujhe\",\n",
    "    \"love\": \"pasand\",\n",
    "    \"eating\": \"khana\",\n",
    "    \"spicy\": \"teekha\",\n",
    "    \"food\": \"khana\",\n",
    "    \"during\": \"ke dinon mein\",\n",
    "    \"winter\": \"sardi\",\n",
    "    \"she\": \"woh\",\n",
    "    \"is\": \"hai\",\n",
    "    \"reading\": \"padh rahi\",\n",
    "    \"book\": \"kitaab\",\n",
    "    \"in\": \"mein\",\n",
    "    \"park\": \"park\",\n",
    "            \"he\": \"woh\",\n",
    "    \"went\": \"gaya\",\n",
    "    \"to\": \"ke liye\",\n",
    "    \"school\": \"school\",\n",
    "    \"learn\": \"seekhne\",\n",
    "    \"mathematics\": \"ganit\",\n",
    "         \"they\": \"woh\",\n",
    "    \"are\": \"hain\",\n",
    "    \"watching\": \"dekh rahe\",\n",
    "    \"movie\": \"film\",\n",
    "    \"at\": \"par\",\n",
    "    \"home\": \"ghar\",\n",
    "    \"tonight\": \"aaj raat\",\n",
    "          \"we\": \"hum\",\n",
    "    \"are\": \"hain\",\n",
    "    \"going\": \"ja rahe\",\n",
    "    \"to\": \"par\",\n",
    "    \"beach\": \"samudra tat\",\n",
    "    \"enjoy\": \"maza lene\",\n",
    "    \"sunset\": \"suryast\",\n",
    "        \"my\": \"mere\",\n",
    "    \"father\": \"pita\",\n",
    "    \"is\": \"hain\",\n",
    "    \"working\": \"kaam kar rahe\",\n",
    "    \"on\": \"par\",\n",
    "    \"big\": \"bade\",\n",
    "    \"project\": \"project\",\n",
    "    \"in\": \"mein\",\n",
    "    \"office\": \"daftar\",\n",
    "            \"he\": \"usko\",\n",
    "    \"likes\": \"pasand\",\n",
    "    \"traveling\": \"ghoomna\",\n",
    "    \"to\": \"par\",\n",
    "    \"new\": \"naye\",\n",
    "    \"places\": \"jagahon\",\n",
    "    \"meeting\": \"milna\",\n",
    "    \"people\": \"logon\",\n",
    "          \"dog\": \"kutta\",\n",
    "    \"is\": \"hai\",\n",
    "    \"barking\": \"bhauk raha\",\n",
    "    \"loudly\": \"zor se\",\n",
    "    \"near\": \"ke paas\",\n",
    "    \"house\": \"ghar\",\n",
    "          \"she\": \"woh\",\n",
    "    \"wants\": \"chahti\",\n",
    "    \"to\": \"ke liye\",\n",
    "    \"bake\": \"banana\",\n",
    "    \"chocolate\": \"chocolate\",\n",
    "    \"cake\": \"cake\",\n",
    "    \"for\": \"ke liye\",\n",
    "    \"friend\": \"dost\"\n",
    "}\n",
    "    \n",
    "    \n",
    "    # Initialize code-switch generator\n",
    "    code_switcher = NaturalCodeSwitcher(word_alignments, switch_probability=0.5)\n",
    "    \n",
    "    # Test sentences\n",
    "    matrix_sentence = \"I love eating spicy food during winter. She is reading a book in the park while the kids are playing in the garden with their friends. He went to school to learn mathematics, and my father is working on a big project in his office. They are watching a movie at home tonight. Meanwhile, the dog is barking loudly near the house. We are going to the beach to enjoy the sunset. Later, she wants to bake a chocolate cake for her friend. Tomorrow, I am going to the market to buy some fruits.\"\n",
    "    embedded_sentence = \"Mujhe tikha khana pasand hai sardi ke dinon mein. Woh park mein ek kitaab padh rahi hai jab bachche bagiche mein apne doston ke saath khel rahe hain. Woh school gaya tha ganit seekhne ke liye, aur mere pita daftar mein ek bade project par kaam kar rahe hain. Woh aaj raat ghar par ek film dekh rahe hain. Isi beech, kutta ghar ke paas zor se bhauk raha hai. Hum samudra tat par suryast ka maza lene ja rahe hain. Baad mein, woh apne dost ke liye ek chocolate cake banana chahti hai. Kal, main bazar ja raha hoon kuchh phal kharidne ke liye..\"\n",
    "    \n",
    "    # Generate code-switched sentence\n",
    "    code_switched = code_switcher.generate_code_switched_sentence(\n",
    "        matrix_sentence, \n",
    "        embedded_sentence\n",
    "    )\n",
    "    \n",
    "    print(\"Matrix Language Sentence:\", matrix_sentence)\n",
    "    print(\"Embedded Language Sentence:\", embedded_sentence)\n",
    "    print(\"Code-Switched Sentence:\", code_switched)\n",
    "\n",
    "# Uncomment to run\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6e070",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 37.25382,
   "end_time": "2024-11-27T20:09:33.863243",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-27T20:08:56.609423",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
